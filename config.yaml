# ============================================================
# OmniFlow Configuration
# ============================================================

# ----------------------
# Model Architecture
# ----------------------
model:
  d_model: 256
  nhead: 8
  num_layers: 4
  measure_dim: 256        # 如果为 null，使用 d_model
  dropout: 0.1
  share_layers: false
  adapter_hidden: 512

# ----------------------
# Geometry (α-divergence)
# ----------------------
geometry:
  alpha_init:
    vis: 0.0
    aud: 0.0
    txt: 0.0
  p_min: 1.001
  p_max: 50.0
  eps: 1.0e-8
  m_min: 1.0e-3
  m_max: 1.0e+3
  w_max: 1.0e+6
  prior_scale: 1.0
  prior_type: "lognormal"    # "lognormal" 或 "softplus"

# ----------------------
# Masking
# ----------------------
masking:
  span_len:
    vis: 6
    aud: 6
    txt: 3
  w_masked: 1.0
  w_visible: 0.05

# ----------------------
# Flow
# ----------------------
flow:
  t_max: 1.0
  t_gamma: 1.0              # <1 偏向 t=1
  normalize_by_geometry: true

# ----------------------
# Text Adapter (Gumbel-Softmax)
# ----------------------
text_adapter:
  txt_usage_weight: 0.05
  use_gumbel: true

# ----------------------
# Representation Extraction
# ----------------------
representation:
  # Mode choices:
  #   - hidden_mean            : mean-pooled hidden states only (d_model * 3)
  #   - hidden_attn            : attention-pooled hidden states only (d_model * 3)
  #   - hidden_attn_vel_x1     : hidden + velocity at x1 (d_model * 6)
  #   - hidden_attn_vel_detprior : hidden + velocity at x_t with deterministic prior (d_model * 6)
  mode: "hidden_attn"

  # Whether to project velocity u_hat (measure_dim) -> d_model before pooling/concat
  vel_proj: true

  # LayerNorm on per-modality concatenated feature [h, v]
  use_layernorm: true

  # Time selection for supervised_ft and classification (unified)
  # If use_multi_t is true, use t_list with optional t_weights; otherwise use single t_star
  use_multi_t: false
  t_star: 1.0
  t_list: [0.7, 0.85, 1.0]
  t_weights: [0.2, 0.3, 0.5]    # null or list; if null, uses uniform weights

# ----------------------
# Training
# ----------------------
training:
  batch_size: 32
  weight_decay: 1.0e-4
  grad_clip: 1.0

  # Stage 1: Geometry Learning
  stage1:
    max_epochs: 10
    patience: 12
    base_lr: 1.0e-4
    alpha_lr: 5.0e-3
    cross_attention: false
    t_gamma: 0.5
    mask_ratio:
      vis: 0.6
      aud: 0.6
      txt: 0.4
    w_visible: 0.0
    txt_usage_weight: 0.05
    gumbel:
      tau_start: 1.0
      tau_end: 0.5

  # Stage 2: Joint Training
  stage2:
    max_epochs: 50
    patience: 18
    lr: 1.0e-4
    cross_warmup: 0
    t_gamma: 0.5
    mask_ratio:
      vis: 0.5
      aud: 0.5
      txt: 0.15
    w_visible: 0.05
    txt_usage_weight: 0.03
    gumbel:
      tau_start: 0.6
      tau_end: 0.3
    asym_mask:
      enabled: true
      low_ratio: 0.1
      high_ratio_vis_aud: 0.6
      high_ratio_txt: 0.4

  # Classification
  classification:
    max_epochs: 100
    patience: 15
    lr: 1.0e-3
    hidden_dims: [256, 128]
    dropout: 0.4

  # Supervised Finetune
  supervised_ft:
    max_epochs: 20
    patience: 8

    unfreeze_last_k: 2
    train_vf_inout_proj: true

    freeze_adapters: false          # 关键：必须是 false
    unfreeze_projs: true
    unfreeze_adapters_partial: true

    lr_vf: 3.0e-5
    lr_projs: 1.0e-4
    lr_adapters: 3.0e-5
    lr_head: 1.0e-3

    weight_decay: 1.0e-4
    log_grads: true



# ----------------------
# Dataset
# ----------------------
dataset:
  # Dataset name: mosei, mosi, urfunny, iemocap, generic
  name: "urfunny"
  
  # Task type (for MOSEI only): "SEN" (sentiment) or "EMO" (emotion)
  task: "SEN"
  
  # Number of classes:
  #   - MOSEI SEN: 2/3/5/7
  #   - MOSEI EMO: 6 (fixed)
  #   - MOSI: 2/3/5/7
  #   - UR-FUNNY: 2 (fixed)
  #   - IEMOCAP: 4/6
  num_classes: 2
  
  # Number of data loading workers (0 = main process)
  num_workers: 4
  
  # Pin memory for GPU transfer
  pin_memory: false
  
  # Which modalities to normalize (default: ["vis", "aud"])
  normalize: ["vis", "aud"]
  
  # Label key in data (for generic dataset)
  label_key: "labels"
  
  # Modality keys for generic dataset (optional override)
  modality_keys:
    urfunny:
      vis: "vision"
      aud: "audio"
      txt: "text"

